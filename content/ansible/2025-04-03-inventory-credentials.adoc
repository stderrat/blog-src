---
title: "Using custom credential types for securing inventory secrets"
description: "This post tries to explain how leverage Ansible Automation Platform custom credential types to encrypt secrets required in custom inventory plugins"
date: "2025-04-03"
doctype: book

authors: [Toni Schmidbauer]
type: post
draft: true
categories:
   - Ansible
   - Credentials
   - Iventory
tags: ["Ansible", "Credentials", "Inventory"]

aliases: [
	 "/posts-output/2025-04-03-inventory-credentials/",
	 "/ansible/2025/04/inventory-credentials/",
]
---

:imagesdir: /ansible/images/
:icons: font
:toc:

If you are going to write a custom inventory plugin, there's a chance that you need to use credential's for connecting to an external source for your inventory. It's not a good idea to store these credentials in the inventory, either as an inventory variable or directly in the configuration file.

This blog post explains how you can use custom _credential types_ in Ansible Automation Platform to safely store those secrets.

<!--more-->

# Step 1: Create a custom credential type

To read up on credential types go ahead and read the https://docs.redhat.com/en/documentation/red_hat_ansible_automation_platform/2.5/html/using_automation_execution/assembly-controller-custom-credentials#proc-controller-create-credential-type[Automation Platform documentation].

The following screenshot depicts required settings we need to add to Ansible Automation Platform (AAP) under `Automation Exection / Infrastructure / Credential Types`:

image::aap_credential_type.png[]

With the custom credential type created we can create a new credential of that type (Example Inventory). AAP is going to ask for a username (of type string) and a password. The password entry is going to be hidden because we've set `secret` to `true`. Both fields, username and password are also required fields.

So let's go ahead and let's create an actual credential of that type.

# Step 2: Creating a credential of type `Example Inventory`

Move to `Automation Execution / Infrastructure / Credentials` and create the following credential

image::aap_credential.png[]

Important is selecting our custom credential type `Example Inventory` under `Credential Type`.

You can see that the Username and Password fields are required and the Password entry field is hidden per default. This is what we specified when we created our custom credential type in step 1.

# Step 3: Create an example inventory plugin using the credentials from step 2

For demoing using the custom credential type we created in step 1 we created an example inventory plugin that uses the variables `EXAMPLE_INVENTORY_USERNAME` and `EXAMPLE_INVENTORY_PASSWORD.

The source code for this plugin is available at https://github.com/tosmi-ansible/ansible-playground/blob/main/plugins/inventory/example.py.

Our custom credential type will pass the username and the password as environment variables to the inventory plugin. So we use the Python function `os.environ.get()` to read those two variables (lines 35 and 36):

[source,python]
username = os.environ.get('EXAMPLE_INVENTORY_USERNAME')
password = os.environ.get('EXAMPLE_INVENTORY_PASSWORD')

Just for testing (please don't do this in a production grade inventory plugin), we display the variable values and also add both variables to a group defined in the inventory (lines 57 and 58):

[source,python]
self.inventory.set_variable(group, 'inventory_username', username)
self.inventory.set_variable(group, 'inventory_password', password)

This repository also includes an inventory to be used with our custom inventory plugin it's located at https://github.com/tosmi-ansible/ansible-playground/blob/main/inventory/example.yaml.


# Step 4: Create an Ansible Automation Platform inventory using the plugin

Because we do everything as configuration as code, the following code snippet will add an inventory using our plugin to Ansible Automation Platform:



[source,yaml]
test

NOTE: Ignition is going to apply a configuration only on the first machine boot. This is different to cloud-init. Cloud init applies provided configurations on every boot.

See the https://coreos.github.io/ignition/rationale/#ignition-runs-only-on-the-first-boot[rational] for more information

# The problem statement

So we wanted to leverage ignition for customizing worker nodes running on OpenShift Virtualization (OCP-V). Basically we use it for network configuration and to point it to the OpenShift machine config server which provides an ignition configuration for adding machines to a running cluster.

WARNING: Using Ignition in this way is probably an unsupported way of setting up worker nodes for an OpenShift cluster. According to the documentation only modifying the installation ISO or PXE booting nodes is supported. And in the end it didn't work for us, but this is a different story.

KubeVirt seems to https://kubevirt.io/user-guide/user_workloads/startup_scripts/#ignition-examples[support] specifying ignition configuration in a VirtualMachine object.

We downloaded the qemu qcow2 https://mirror.openshift.com/pub/openshift-v4/amd64/dependencies/rhcos/4.18/latest/rhcos-qemu.x86_64.qcow2.gz[image], uploaded it to a KubeVirt DataVolume via

[source]
virtctl image-upload --force-bind  dv rhcos-4.17.17 --size=18Gi --image-path=rhcos-4.17.17-x86_64-qemu.x86_64.qcow2 --insecure

Next we tried to create a VirtualMachine:

[source]
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  labels:
    app: rhcos
  name: rhcos-vm
  namespace: test-vms
spec:
  dataVolumeTemplates:
    - apiVersion: cdi.kubevirt.io/v1beta1
      kind: DataVolume
      metadata:
        name: rhcos-vm
      spec:
        source:
          pvc:
            name: rhcos-4.17.17 <1>
            namespace: test-vms
        storage:
          resources:
            requests:
              storage: 100Gi
  running: true
  template:
    spec:
      accessCredentials:
        - sshPublicKey:
            propagationMethod:
              noCloud: {}
            source:
              secret:
                secretName: tonipub
      architecture: amd64
      domain:
        cpu:
          cores: 1
          sockets: 1
          threads: 1
        devices:
          disks:
            - disk:
                bus: virtio
              name: rootdisk
            - disk:
                bus: virtio
              name: cloudinitdisk
          interfaces:
            - bridge: {}
              macAddress: '02:e0:d7:00:00:06'
              model: virtio
              name: nic-peach-crayfish-71
          rng: {}
        features:
          acpi: {}
          smm:
            enabled: true
        firmware:
          bootloader:
            efi: {}
        machine:
          type: pc-q35-rhel9.4.0
        memory:
          guest: 2Gi
        resources: {}
      networks:
        - multus:
            networkName: default/bridge-network
          name: nic-peach-crayfish-71
      terminationGracePeriodSeconds: 180
      volumes:
        - dataVolume:
            name: rhcos-vm
          name: rootdisk
        - name: cloudinitdisk
          cloudInitNoCloud:
            userData: |- <2>
              {
                "ignition": {
                  "config": {},
                  "proxy": {},
                  "security": {},
                  "timeouts": {},
                  "version": "3.4.0"
                },
                "passwd": {
                  "users": [
                    {
                      "name": "coreos",
                      "sshAuthorizedKeys": [
                        "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIYhjnWzsArZVyyTa1E6sDbH06rUGDAhAF3bf3pmeBtm toni@stderr.at"
                      ],
                      "name": "toni",
                      "sshAuthorizedKeys": [
                        "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIYhjnWzsArZVyyTa1E6sDbH06rUGDAhAF3bf3pmeBtm toni@stderr.at"
                      ]
                    }
                  ]
                },
                "storage": {},
                "systemd": {}
              }

<1> Use the uploaded image as a template for the new VM
<2> This is the igntion config that we would like to use

But after booting the VM we only saw

[source]
XXX message missing

on the VM console.

The question is why? This is a CoreOS based VM, it runs igntion on first boot, but obviously does not apply our configuration.

# Diving into ignition

So it's time get a detailed understanding how the igintion process really works.

We already now that ignition configuration is applied early in the boot process. This is done by including ignition in the initial ramdisk.

So step one is to extract the ramdisk and take a look at what is going on.

What's an initial ram filesystem aka initramfs? Let's ask Claude 3.7 Sonnet:

NOTE: The Linux initramfs is a temporary root filesystem loaded into memory during boot that contains essential drivers, modules, and tools needed to mount the actual root filesystem before transitioning control to the final operating system.

Thanks Claude 3.7 Sonnet.

## Extracting the RHCOS intial ramdisk from the qcow image

First we need to somehow access the files in the qcow2 image. This is done via the following steps:

[start=1]
. Load the NBD module and mount the QCOW2 image as a network block device

[source, bash]
sudo modprobe nbd
sudo qemu-nbd --connect /dev/nbd0 rhcos-qemu.x86_64.qcow2

[start=2]
. Lets see which partition we've got

[source]
sudo fdisk -l /dev/nbd0

[start=3]
. Mount the partition to /mnt

[source]
sudo mount -o ro /dev/nbd0p2 /mnt/

[start=4]
. Copy the initramfs file to a tmp location

[source]
df

[start=5]

. Unmount and disconnect the NBD device

[source]
sudo umount /mnt
sudo qemu-nbd -d /dev/nbd0

## Extracting the initramfs file

CoreOS uses dracut for creating the initial ramdisk. So the file format is a little bit special.

## Understanding the ignition configuration

# Conclusio

Depending on ignition provider ignition searches for a valid ocnfiguration.

